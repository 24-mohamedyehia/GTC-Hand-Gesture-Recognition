# GTC-Hand-Gesture-Recognition
This project is an AI-based system designed to recognize hand gestures from static images or live video streams. It aims to enable touchless control and enhance applications such as human-computer interaction, gaming, and sign language interpretation.

## ScreenShots
![ScreenShot](./public/Screenshot.png)

## Features
- Real-time hand gesture recognition via webcam
- Training and evaluation scripts with clear CLI entry points
- Pretrained MobileNetV2 weights included for quick demo
- Results artifacts: confusion matrix and classification report


## ğŸš€ Quickstart

1- Fork and Clone repo
 - `git clone https://github.com/24-mohamedyehia/GTC-Hand-Gesture-Recognition`

2- ğŸ“¦ Install Python Using Miniconda
 - Download and install MiniConda from [here](https://www.anaconda.com/docs/getting-started/miniconda/main#quick-command-line-install)

3- Create a new environment using the following command:
```bash
conda create --name Hand-Gesture-Recognition python=3.11 -y
```

4- Activate the environment:
```bash
conda activate Hand-Gesture-Recognition
```

5- Install the required packages
```bash
pip install -r requirements.txt
```

6- Setup the environment variables
```bash
cp .env.example .env
```
## Demo Video
- Watch a sample demo locally: [Hand_Gesture_recognition.mp4](https://github.com/24-mohamedyehia/GTC-Hand-Gesture-Recognition/raw/refs/heads/main/public/Hand_Gesture_recognition.mp4)
- Tip: GitHub may not stream MP4 in-browser. If playback fails, download the file or convert to MP4 for embedding.

## Presentation
  - Canva : [Presentation (Canva)](https://www.canva.com/design/DAG0GoYWolQ/xzrvgT9eK9UnoKCGMwtb1g/edit)
  - Powerpoint : [Presentation (Powerpoint)](./public/Hand-Gesture-Recognition.pptx.pptx)

## Dataset
We used the ASL Alphabet dataset for training and evaluating our hand gesture recognition model.
You can download the dataset from [Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)

## Project Structure
```
GTC-Hand-Gesture-Recognition/
â”‚
â”œâ”€â”€ Data/                      # Dataset root (train/val)
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ test/
â”‚   â””â”€â”€ val/
â”‚
â”‚
â”œâ”€â”€ preprocessing/           # Data cleaning & augmentation scripts
â”‚   â”œâ”€â”€ dataloader.py
â”‚   â”œâ”€â”€ transforms.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ models/                  # Architectures & model builders
â”‚   â”œâ”€â”€ mobilenet.py         # MobileNetV2 backbone
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/                 # Training / Evaluation / Prediction
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ predict_simple.py
â”‚   â”œâ”€â”€ predict_realtime.py
â”‚   â””â”€â”€ split_data.py
â”œâ”€â”€ outputs/                 # Saved results (checkpoints, plots, logs)
â”‚   â”œâ”€â”€ classification_report.csv
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â””â”€â”€ mobilenetv2_gesture.pth
â”‚
â”œâ”€â”€ utils/                   # Helper functions       
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ train_utils.py
â”‚
â”œâ”€â”€ deployment/              # Apps / serving
â”‚   â””â”€â”€ app.py              
â”‚
â”œâ”€â”€ labels.json              # Mapping: class â†” index
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example        
â”œâ”€â”€ README.md                # Main documentation
â””â”€â”€ .gitignore
```
## Technical Stack
 - Python 3.11
 - Pytorch
 - OpenCV
 - streamlit
 - Scikit-learn

## Authors
- [Mohamed Yehia](https://github.com/24-mohamedyehia)
- [Ahmed Ammar](https://github.com/a7med-3mmar)
- [Faris Abouagour](https://github.com/faris-agour)
- [Mohamed Ghoneim](https://github.com/mohamed-aliii)
- [Youssef Fady](https://github.com/Youssefady)

## ğŸ“œ License
This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.

## ğŸ“ Acknowledgments
- [ASL Alphabet Dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)
- [GTC](https://www.linkedin.com/company/genius-technology-center/posts/?feedView=all)

![Python](https://img.shields.io/badge/python-3.11-blue)
![MIT](https://img.shields.io/badge/MIT-green)
![Miniconda](https://img.shields.io/badge/Miniconda-FF6C37.svg?style=for-the-badge&logo=Miniconda&logoColor=black)
![Streamlit](https://img.shields.io/badge/Streamlit-FF6C37.svg?style=for-the-badge&logo=Streamlit&logoColor=black)
![PyTorch](https://img.shields.io/badge/PyTorch-FF6C37.svg?style=for-the-badge&logo=PyTorch&logoColor=black)
![OpenCV](https://img.shields.io/badge/OpenCV-FF6C37.svg?style=for-the-badge&logo=OpenCV&logoColor=black)
![GitHub](https://img.shields.io/badge/GitHub-FF6C37.svg?style=for-the-badge&logo=GitHub&logoColor=black)
![Git](https://img.shields.io/badge/Git-FF6C37.svg?style=for-the-badge&logo=Git&logoColor=black)
![Kaggle](https://img.shields.io/badge/Kaggle-FF6C37.svg?style=for-the-badge&logo=Kaggle&logoColor=black)


