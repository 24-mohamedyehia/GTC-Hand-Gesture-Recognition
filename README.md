# GTC-Hand-Gesture-Recognition
This project is an AI-based system designed to recognize hand gestures from static images or live video streams. It aims to enable touchless control and enhance applications such as human-computer interaction, gaming, and sign language interpretation.
  
## Project Structure
```
GTC-Hand-Gesture-Recognition/
â”‚
â”œâ”€â”€ data/ 
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ test/
â”‚   â””â”€â”€ val/
â”‚
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”‚   â”œâ”€â”€ Hand_Gesture.ipynb  
â”‚   â””â”€â”€ experiments.ipynb
â”‚
â”œâ”€â”€ preprocessing/           # Data cleaning & augmentation scripts
â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â”œâ”€â”€ augmentation.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ models/                  # Architectures
â”‚   â”œâ”€â”€ mobilenet.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ scripts/                 # Training / Evaluation / Prediction
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ predict_simple.py
â”‚   â””â”€â”€ predict_realtime.py
â”‚
â”œâ”€â”€ outputs/                 # Saved results (checkpoints, plots, logs)
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ utils/                   # Helper functions
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ deployment/              # Apps / serving
â”‚   â””â”€â”€ app.py              
â”‚
â”œâ”€â”€ labels.json              # Mapping: class â†” index
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # Main documentation
â””â”€â”€ .gitignore
```


## ðŸš€ Quickstart

1- Fork and Clone repo
 - `git clone https://github.com/24-mohamedyehia/GTC-Hand-Gesture-Recognition`

2- ðŸ“¦ Install Python Using Miniconda
 - Download and install MiniConda from [here](https://www.anaconda.com/docs/getting-started/miniconda/main#quick-command-line-install)

3- Create a new environment using the following command:
```bash
conda create --name Hand-Gesture-Recognition python=3.11 -y
```

4- Activate the environment:
```bash
conda activate Hand-Gesture-Recognition
```

5- Install the required packages
```bash
pip install -r requirements.txt
```

6- Setup the environment variables
```bash
cp .env.example .env
```
## Dataset
We used the ASL Alphabet dataset for training and evaluating our hand gesture recognition model.
You can download the dataset from [Kaggle](https://www.kaggle.com/datasets/grassknoted/asl-alphabet)

## Authors
- [Mohamed Yehia](https://github.com/24-mohamedyehia)
- [Ahmed Ammar](https://github.com/a7med-3mmar)
- [Faris Abouagour](https://github.com/faris-agour)
- [Mohamed Ghoneim](https://github.com/mohamed-aliii)
- [Youssef Fady](https://github.com/Youssefady)

## ðŸ“œ License
This project is licensed under the MIT License. See the [LICENSE](./LICENSE) file for details.
